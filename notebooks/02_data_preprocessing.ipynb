{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Power Prediction - Data Preprocessing\n",
    "\n",
    "This notebook handles data cleaning, feature engineering, and preparation for machine learning.\n",
    "\n",
    "## Objectives\n",
    "- Load and merge multiple data sources\n",
    "- Handle missing values and outliers\n",
    "- Engineer time-based and lag features\n",
    "- Prepare data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Integration\n",
    "\n",
    "In a full implementation, you would load multiple CSV files. For this demo, we'll work with the processed sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sample_data():\n",
    "    \"\"\"Load or create sample data for demonstration\"\"\"\n",
    "    try:\n",
    "        # Try to load existing processed data\n",
    "        data = pd.read_csv('../data/processed_solar_sample.csv')\n",
    "        print(f\"Loaded existing data: {data.shape}\")\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(\"Creating sample dataset for demonstration...\")\n",
    "        \n",
    "        # Create realistic sample data\n",
    "        np.random.seed(42)\n",
    "        n_samples = 5000\n",
    "        \n",
    "        # Generate time series\n",
    "        dates = pd.date_range('2023-01-01', periods=n_samples, freq='H')\n",
    "        \n",
    "        # Generate realistic solar patterns\n",
    "        hours = dates.hour\n",
    "        days = dates.dayofyear\n",
    "        \n",
    "        # Solar irradiance with daily and seasonal patterns\n",
    "        irradiance = np.maximum(0, \n",
    "            800 * np.sin(np.pi * (hours - 6) / 12) * \n",
    "            np.sin(np.pi * days / 365) + \n",
    "            np.random.normal(0, 100, n_samples)\n",
    "        )\n",
    "        \n",
    "        # Temperature with seasonal variation\n",
    "        temperature = 20 + 10 * np.sin(2 * np.pi * days / 365) + np.random.normal(0, 3, n_samples)\n",
    "        \n",
    "        # Power generation based on irradiance with some noise\n",
    "        power = np.maximum(0, irradiance * 3 + temperature * 10 + np.random.normal(0, 200, n_samples))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        data = pd.DataFrame({\n",
    "            'Time': dates,\n",
    "            'Power_W': power,\n",
    "            'Irradiance': irradiance,\n",
    "            'Temperature': temperature,\n",
    "            'RelativeHumidity': np.random.normal(60, 15, n_samples),\n",
    "            'WindSpeed': np.random.exponential(5, n_samples),\n",
    "            'Station': np.random.choice(['Station_A', 'Station_B', 'Station_C'], n_samples)\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Load data\n",
    "data = load_sample_data()\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Columns: {list(data.columns)}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(f\"Missing values:\")\n",
    "for col, missing in missing_values.items():\n",
    "    if missing > 0:\n",
    "        print(f\"  {col}: {missing} ({missing/len(data)*100:.1f}%)\")\n",
    "    \n",
    "if missing_values.sum() == 0:\n",
    "    print(\"  No missing values found!\")\n",
    "\n",
    "# Data types\n",
    "print(f\"\\nData types:\")\n",
    "print(data.dtypes)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nBasic statistics:\")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df, column):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Check for outliers in key columns\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "outlier_summary = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outliers, lower, upper = detect_outliers_iqr(data, col)\n",
    "    outlier_summary[col] = {\n",
    "        'count': len(outliers),\n",
    "        'percentage': len(outliers) / len(data) * 100,\n",
    "        'bounds': (lower, upper)\n",
    "    }\n",
    "\n",
    "print(\"OUTLIER ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "for col, info in outlier_summary.items():\n",
    "    print(f\"{col}: {info['count']} outliers ({info['percentage']:.1f}%)\")\n",
    "\n",
    "# Visualize outliers for Power_W\n",
    "if 'Power_W' in data.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Box plot\n",
    "    axes[0].boxplot(data['Power_W'].dropna())\n",
    "    axes[0].set_title('Power Generation - Box Plot')\n",
    "    axes[0].set_ylabel('Power (W)')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram\n",
    "    axes[1].hist(data['Power_W'].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Power Generation - Distribution')\n",
    "    axes[1].set_xlabel('Power (W)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(df, time_col='Time'):\n",
    "    \"\"\"Create time-based features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure time column is datetime\n",
    "    if time_col in df.columns:\n",
    "        df[time_col] = pd.to_datetime(df[time_col])\n",
    "        \n",
    "        # Basic time features\n",
    "        df['Hour'] = df[time_col].dt.hour\n",
    "        df['Day'] = df[time_col].dt.day\n",
    "        df['Month'] = df[time_col].dt.month\n",
    "        df['DayOfWeek'] = df[time_col].dt.dayofweek\n",
    "        df['DayOfYear'] = df[time_col].dt.dayofyear\n",
    "        df['Weekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        df['Hour_sin'] = np.sin(2 * np.pi * df['Hour'] / 24)\n",
    "        df['Hour_cos'] = np.cos(2 * np.pi * df['Hour'] / 24)\n",
    "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        df['DayOfYear_sin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365)\n",
    "        df['DayOfYear_cos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365)\n",
    "        \n",
    "        # Season\n",
    "        df['Season'] = df['Month'].map({\n",
    "            12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "            3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "            6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "            9: 'Autumn', 10: 'Autumn', 11: 'Autumn'\n",
    "        })\n",
    "        \n",
    "        # Simple solar elevation (approximation)\n",
    "        df['SolarElevation'] = np.maximum(0, \n",
    "            np.sin(np.pi * (df['Hour'] - 6) / 12) * \n",
    "            np.sin(2 * np.pi * df['DayOfYear'] / 365)\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create time features\n",
    "data_with_time = create_time_features(data)\n",
    "print(f\"Added time features. New shape: {data_with_time.shape}\")\n",
    "print(f\"New columns: {[col for col in data_with_time.columns if col not in data.columns]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, target_col='Power_W', lags=[1, 24]):\n",
    "    \"\"\"Create lag features for time series\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if target_col in df.columns:\n",
    "        for lag in lags:\n",
    "            df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag)\n",
    "        \n",
    "        # Rolling statistics\n",
    "        df[f'{target_col}_rolling_mean_6'] = df[target_col].rolling(window=6, min_periods=1).mean()\n",
    "        df[f'{target_col}_rolling_std_6'] = df[target_col].rolling(window=6, min_periods=1).std()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create lag features\n",
    "data_with_lags = create_lag_features(data_with_time)\n",
    "print(f\"Added lag features. New shape: {data_with_lags.shape}\")\n",
    "print(f\"Lag columns: {[col for col in data_with_lags.columns if 'lag' in col or 'rolling' in col]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_engineered_features(df):\n",
    "    \"\"\"Create domain-specific engineered features\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Power density (efficiency metric)\n",
    "    if 'Power_W' in df.columns and 'Irradiance' in df.columns:\n",
    "        df['Power_Density'] = df['Power_W'] / (df['Irradiance'] + 1e-6)\n",
    "    \n",
    "    # Temperature efficiency factor\n",
    "    if 'Temperature' in df.columns:\n",
    "        df['Temp_Efficiency'] = 1 - 0.004 * (df['Temperature'] - 25)  # Typical solar panel temp coefficient\n",
    "    \n",
    "    # Clear sky index (if we had theoretical max irradiance)\n",
    "    if 'Irradiance' in df.columns and 'SolarElevation' in df.columns:\n",
    "        theoretical_max = 1000 * df['SolarElevation']  # Simplified\n",
    "        df['Clear_Sky_Index'] = df['Irradiance'] / (theoretical_max + 1e-6)\n",
    "    \n",
    "    # Station encoding (if categorical)\n",
    "    if 'Station' in df.columns:\n",
    "        station_mapping = {station: i for i, station in enumerate(df['Station'].unique())}\n",
    "        df['Station_encoded'] = df['Station'].map(station_mapping)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create engineered features\n",
    "data_engineered = create_engineered_features(data_with_lags)\n",
    "print(f\"Added engineered features. Final shape: {data_engineered.shape}\")\n",
    "print(f\"Engineered columns: {[col for col in data_engineered.columns if col in ['Power_Density', 'Temp_Efficiency', 'Clear_Sky_Index', 'Station_encoded']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (created by lag features)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_before = data_engineered.isnull().sum().sum()\n",
    "print(f\"Missing values before cleaning: {missing_before}\")\n",
    "\n",
    "# Drop rows with missing lag features (first few rows)\n",
    "data_clean = data_engineered.dropna()\n",
    "\n",
    "missing_after = data_clean.isnull().sum().sum()\n",
    "print(f\"Missing values after cleaning: {missing_after}\")\n",
    "print(f\"Rows removed: {len(data_engineered) - len(data_clean)}\")\n",
    "print(f\"Final dataset shape: {data_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove extreme outliers (optional)\n",
    "def remove_outliers(df, columns, method='iqr', factor=1.5):\n",
    "    \"\"\"Remove outliers using IQR method\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col in df_clean.columns:\n",
    "            Q1 = df_clean[col].quantile(0.25)\n",
    "            Q3 = df_clean[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - factor * IQR\n",
    "            upper_bound = Q3 + factor * IQR\n",
    "            \n",
    "            before_count = len(df_clean)\n",
    "            df_clean = df_clean[(df_clean[col] >= lower_bound) & (df_clean[col] <= upper_bound)]\n",
    "            after_count = len(df_clean)\n",
    "            \n",
    "            print(f\"{col}: Removed {before_count - after_count} outliers\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Remove outliers from key columns\n",
    "outlier_columns = ['Power_W', 'Irradiance', 'Temperature']\n",
    "data_final = remove_outliers(data_clean, outlier_columns, factor=2.0)  # More conservative\n",
    "\n",
    "print(f\"\\nFinal dataset shape after outlier removal: {data_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "target_column = 'Power_W'\n",
    "exclude_columns = ['Time', 'Station', 'Season']  # Non-numeric or identifier columns\n",
    "\n",
    "# Select feature columns\n",
    "feature_columns = [col for col in data_final.columns \n",
    "                  if col != target_column and col not in exclude_columns]\n",
    "\n",
    "print(f\"Target variable: {target_column}\")\n",
    "print(f\"Number of features: {len(feature_columns)}\")\n",
    "print(f\"Features: {feature_columns}\")\n",
    "\n",
    "# Prepare feature matrix and target vector\n",
    "X = data_final[feature_columns]\n",
    "y = data_final[target_column]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Training target: {y_train.shape}\")\n",
    "print(f\"Test target: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled test features shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed dataset\n",
    "processed_data = data_final.copy()\n",
    "processed_data.to_csv('../data/processed_solar_data.csv', index=False)\n",
    "\n",
    "# Save feature information\n",
    "feature_info = {\n",
    "    'target': target_column,\n",
    "    'features': feature_columns,\n",
    "    'categorical': ['Station_encoded'],  # Categorical features that were encoded\n",
    "    'numerical': [col for col in feature_columns if col != 'Station_encoded'],\n",
    "    'n_samples': len(processed_data),\n",
    "    'n_features': len(feature_columns)\n",
    "}\n",
    "\n",
    "with open('../data/feature_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "# Save train/test splits\n",
    "train_data = pd.concat([X_train_scaled, y_train], axis=1)\n",
    "test_data = pd.concat([X_test_scaled, y_test], axis=1)\n",
    "\n",
    "train_data.to_csv('../data/train_data.csv', index=False)\n",
    "test_data.to_csv('../data/test_data.csv', index=False)\n",
    "\n",
    "print(\"Data saved successfully!\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  - ../data/processed_solar_data.csv\")\n",
    "print(f\"  - ../data/feature_info.json\")\n",
    "print(f\"  - ../data/train_data.csv\")\n",
    "print(f\"  - ../data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA PREPROCESSING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Original dataset shape: {data.shape}\")\n",
    "print(f\"Final dataset shape: {processed_data.shape}\")\n",
    "print(f\"Features created: {len(feature_columns)}\")\n",
    "print(f\"Target variable: {target_column}\")\n",
    "\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - Time features: {len([col for col in feature_columns if any(x in col for x in ['Hour', 'Day', 'Month', 'sin', 'cos', 'Solar'])])}\")\n",
    "print(f\"  - Weather features: {len([col for col in feature_columns if any(x in col for x in ['Irradiance', 'Temperature', 'Humidity', 'Wind'])])}\")\n",
    "print(f\"  - Lag features: {len([col for col in feature_columns if 'lag' in col or 'rolling' in col])}\")\n",
    "print(f\"  - Engineered features: {len([col for col in feature_columns if any(x in col for x in ['Density', 'Efficiency', 'Index', 'encoded'])])}\")\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  - Training: {len(X_train)} samples ({len(X_train)/len(processed_data)*100:.1f}%)\")\n",
    "print(f\"  - Testing: {len(X_test)} samples ({len(X_test)/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTarget variable statistics:\")\n",
    "print(f\"  - Mean: {y.mean():.2f}\")\n",
    "print(f\"  - Std: {y.std():.2f}\")\n",
    "print(f\"  - Min: {y.min():.2f}\")\n",
    "print(f\"  - Max: {y.max():.2f}\")\n",
    "\n",
    "print(\"\\nData is ready for model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "The data is now preprocessed and ready for machine learning. The next steps are:\n",
    "\n",
    "1. **Model Training**: Train multiple algorithms (Random Forest, Gradient Boosting, etc.)\n",
    "2. **Model Evaluation**: Compare performance using appropriate metrics\n",
    "3. **Hyperparameter Tuning**: Optimize the best performing models\n",
    "4. **Feature Importance**: Analyze which features are most predictive\n",
    "\n",
    "Continue to the next notebook: `03_model_training.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

